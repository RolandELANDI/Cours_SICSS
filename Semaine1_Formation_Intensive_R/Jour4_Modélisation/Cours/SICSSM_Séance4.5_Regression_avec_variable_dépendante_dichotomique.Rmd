---
title: 'Séance 4.5: Regression avec variable dépendante dichotomique'
subtitle: "Régression logistique"
author: "Visseho Adjiwanou, PhD."
institute: "SICSS - Montréal"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  beamer_presentation:
    colortheme: beaver
    fonttheme: structurebold
    theme: Antibes
  slidy_presentation: default
  ioslides_presentation: default
---


## Plan de présentation

- Rappel
- Introduction
- Variables dépendantes dichotomiques
  - Exemple
  - Estimation:
    - Pourquoi l'OLS n'est pas approprié
    - Logit ou probit?
  - Estimation
  - Interprétation
  - Tests d'hypothèses


Rappel
=============================================================

## Quelle méthode de régression ?

- Le type de méthode dépend du type de la **variable dépendante**

Variables dépendantes          Méthodes
------------------------------ ------------------------------
Quantitatives continues        Régression linéaire
Qualitative dichotomique       Logistique, probit
Qualitative avec plus de       Logit ou probit multinomial
deux catégories nominale        
Qualitative avec plus de       Logit ou probit ordonné
deux catégories (ordinale)        
Durée                          Modèle de durée ou de survie

Introduction
=====================================================

## Introduction

- La variable dépendante dichotomique est un cas particulier de variable dépendante qualitative où la variable dépendante n'a que deux catégories
  - Succès / perte, malade ou non, entrée dans la sexualité ou non
- Variable dépendante qualitative


## Introduction

- L'analyse de régression d'une variable qualitative binaire ou dichotomique est un problème courant en sociologie
- Ces modèles se concentrent sur les déterminants de la probabilité p d'occurrence d'un résultat plutôt que sur un autre résultat qui se produit avec une probabilité de 1-p.
- Exemples:
 - Modéliser si le premier rapport sexuel a eu lieu pendant l'adolescence ou non
 - Modéliser si une personne a utilisé une méthode de contraception moderne ou pas
 - Donnez-moi d'autres exemples

Estimation
==============================================

## Estimation

- Dans l'analyse de régression, nous voulons mesurer comment la probabilité p varie d'un individu à l'autre en fonction des régresseurs.

- Trois principales approches d'estimation sont utilisées:
  1. Le modèle de probabilité linéaire
  - Souvent dans le cas d'un régresseur endogène
  2. Le modèle logit
  3. le modèle probit

## 1. Le modèle de probabilité linéaire

- $Y_i = \beta_0 + \beta_1X_i + ... + \beta_kX_k + \epsilon+j$

- Parce que y ne peut prendre que deux valeurs, $\beta_j$ ne peut pas être interprété comme le changement de y étant donné une augmentation d'une unité de $X_j$, en maintenant tous les autres facteurs fixes: y passe de 0 à 1, ou de 1 à 0 (ou ne change pas).

- Le modèle de régression linéaire multiple avec une variable dépendante binaire est appelé le modèle de probabilité linéaire (LPM) car la probabilité de réponse est linéaire dans les paramètres $\beta_j$.

## 1. Le modèle de probabilité linéaire

![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/lpm.png)

- Il est évident que la droite d'estimation n'est pas appropriée pour traiter la variable dépendante dichotomique

## 1. Le modèle de probabilité linéaire

- Formulation

- $Y_i = \beta_0 + \beta_1X_i + ... + \beta_kX_k + \epsilon+j$
- $Y_i = \beta_0 + \beta_1X_i + \epsilon+j$ (simple régression linéaire)
- $Y = X\beta + \epsilon$ sous la forme matricielle
- $E(Y|X) = \beta_0 + \beta_1X_i + ... + \beta_kX_k$
- Y variable dichotomique ==> P(Y = 1|X) = E(Y|X)
- donc, E(Y|X) est interprété comme une probabilité
- $\beta_j$ mesure le changement de la probabilité de succès lorsque $X_j$ change, en maintenant les autres facteurs fixes

## 1. Le modèle de probabilité linéaire

- Deux principaux problèmes avec le LPM: 
  1. La probabilité prédite est supérieure à 1 ou inférieure à 0
  2. Les termes d'erreurs sont hétéroscédastiques

## 1. Le modèle de probabilité linéaire

1. Les valeurs prédites sont illimitées

![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/lpm1.png)

## 1. Le modèle de probabilité linéaire

2. Les termes d'erreurs sont hétéroscédastiques
- Dans ce cas, $\epsilon_i$ prend deux valeurs:
- $-X\beta$ si Y = 0 avec P(Y = 0) = $1- X\beta$
- $(1-X\beta)$ si Y = 1 avec P(Y = 1) = $X\beta$

Valeur de $\epsilon$       Probabilité
-------------------------- --------------------
$-X\beta$                  $1- X\beta$
$(1-X\beta)$               $X\beta$


- $E(\epsilon) = -X\beta * (1- X\beta) + (1-X\beta) * X\beta$ = 0
- $Var(\epsilon) = (-X\beta)^2 * (1- X\beta) + (1-X\beta)^2 * X\beta = X\beta(1-X\beta)$
==> $Var(\epsilon)$ n'est pas constant

## 1. Le modèle de probabilité linéaire
- Ces deux problèmes ne sont pas insurmontables:
  - Changer la valeur des valeurs prédites
    - 0 pour toutes les valeurs négatives
    - 1 pour toutes les valeurs supérieures à 1

- Estimation en contrôlant l'hétéroscédasticité

## Modèle Logit / Probit

![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/lpm.png)

- Ce qu'il faut, c'est un moyen de presser les probabilités estimées à l'intérieur de l'intervalle 0-1
- $P(Y = 1) = G(Y_i = \beta_0 + \beta_1X_i + ... + \beta_kX_k$)


## Modèle Logit / Probit

- $P(Y = 1) = G(\beta_0 + \beta_1X_i + ... + \beta_kX_k$)

- De nombreuses fonctions sont disponibles
- Les deux plus populaires sont:
  - La fonction normale cumulative qui donne le modèle probit
  - La fonction logistique qui donne le modèle logit

- Le modèle log-log complémentaire pour la distribution non symétrique
- Pour les phénomènes rares, où la probabilité de succès est faible

## Modèle Logit / Probit : Formulation

- Forme latente

- $Y_i^* = \beta_0 + \beta_1X_{1i} + ... + \beta_kX_{ki} + \epsilon_i$, i allant de 1 à n

- On observe :
- $Y_i = 1$ si $Y_i^* > 0$
- $Y_i = 0$ si $Y_i^* < 0$

- $P(Y_i = 1) = P(Y_i^* > 0) = P(X\beta + \epsilon > 0)$
- $P(Y_i = 1) = P(\epsilon > -X\beta)$
- $P(Y_i = 1) = P(\epsilon < X\beta) = \psi(Y_i^*)$
- Où $\psi$ est la fonction de distribution cumulative 

## Modèle Logit / Probit : Estimation

- Les techniques du maximum de vraisemblance sont utilisées pour estimer les paramètres
- Pour chaque observation, la probabilité d'observation Y conditionnelle à X peut s'écrire:
- $P(Y_i = y_i|X) = {\psi(x_i\beta)}^{y_i}{1 - \psi(x_i\beta)}^{1-y_i}$ avec $\text{$y_i$ = 0 ou 1}$

- La vraisemblance logarithmique de l'observation i peut s'écrire:
- $l_i(\beta) = y_ilog[\psi(x_i\beta)] + (1-y_i)log[(1 - \psi(x_i\beta)]$
- Et la vraisemblance de l'échantillon vaut:

$$L(\beta) = \sum l_i(\beta)$$

## Modèle Logit / Probit 

**Logit**

- La fonction de distribution cumulative est la fonction logistique:
- $\psi(t) = \frac{exp(t)}{1 + exp(t)}$
- $P(Y = 1 | x) = \pi_i = \frac{exp(x\beta)}{1 + exp(x\beta)}$
- $logit(\pi_i) = Log(\frac{\pi_i}{1-\pi_i}) = \beta_0 + \beta_1X_{1i} + ... + \beta_kX_{ki}$

## Modèle Logit / Probit 

**Probit**

- La fonction de densité cumulée est la fonction normale:
- $G(t) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{t} e^{-\frac{1}{2}x^2}dx$
- $P(Y = 1) = G(X\beta)$
- $G^{-1}[P = 1] = \beta_0 + \beta_1X_{1i} + ... + \beta_kX_{ki}$

## Modèle Logit / Probit 

- Choix entre les deux modèles:

  - Les deux fonctions sont très similaires
  - Le choix est une question de goût en raison de la disponibilité du logiciel
  - Logit populaire dans la santé publique tandis que probit est plus populaire parmi les économistes
  - Logit facilement manipulable : popularisé par la notion de rapport de chances (odd ratio) 
  
Interprétation  
=============================================  

## Modèle Logit / Probit 

**Logit**

- $Log(\frac{\pi_i}{1-\pi_i}) = \beta_0 + \beta_1X_{1i} + ... + \beta_kX_{ki}$
- $d(Log(\frac{\pi_i}{1-\pi_i})/d(X_{1i}))$ donne $\beta_1$
- On peut démontrer que:

$$d(\pi_i)/d(X_1) = \beta_1\pi_i(1-\pi_i)$$

- $\beta_1$ n'explique pas le changement de probabilité dû à un changement d'unité dans la variable $X_1$
- $d(\pi_i)/d(X_1)$ dépend de la valeur des autres variables du modèle
- Pour interpréter l'effet de X1, il faut aussi fixer $\pi_i(1-\pi_i)$

## Modèle Logit / Probit 

**Probit**

- $P(Y = 1) = \pi_i = G(X\beta)$
- $d(\pi_i)/d(X_1) = \beta_1G^{'}(X\beta)$
- Plus difficile que le modèle logit

## Modèle Logit / Probit 

**Logit** : Interprétation alternative

- Odd ratio ou rapport de chance
- $Log(\frac{\pi_i}{1-\pi_i}) = \beta_0 + \beta_1X_{1i} + ... + \beta_kX_{ki}$
- $\frac{\pi_i}{1-\pi_i} = exp(\beta_0 + \beta_1X_{1i} + ... + \beta_kX_{ki})$
- $\frac{\pi_i}{1-\pi_i} = exp(\beta_0)exp(\beta_1X_{1i})* ... *exp(\beta_kX_{ki})$
- C'est ce qu'on appelle une chance ou une côte
- Si vous faites varier X1 de 0 à 1, vous pouvez calculer le rapport de cette côte.

Autres éléments à considérer
=======================================================

## Violation des hypothèses

- Toute violation d'hypothèse comme dans le cas d'un modèle linéaire affecte les estimations et leur erreur standard
  - Variables omises
  - Hétéroscédasticité
  - Multicolinéarité
  - Endogénéité…

## Mesure de la «qualité d'ajustement»

- En régression linéaire, ce rôle est joué par $R^2$ ou $pseudo-R^2$.
- $R^2$ ou pseudo $R^2$ ne conviennent pas das le cas de modèle logit ou probit

**Alternative**:

- Tableau calculant le nombre de valeurs y = 1 correctement et incorrectement prédites et le nombre de valeurs y = 0 correctement et incorrectement prédites, où une observation est prédite comme y = 1 si la probabilité estimée dépasse une valeur fixe (souvent la moitié)
- Doit être utilisé avec prudence

## Test d'hypothèses

- Test d'hypothèse d'un paramètre
  - t Student est utilisé dans le cas de la modélisation logit
  - la statistique z est utilisée dans le cas de la modélisation probit

- Test de nombreuses hypothèses

  - Test du rapport de vraisemblance (LR) (test de Fischer en cas de régression linéaire)
  - $LR = -2 [L(RM) - L(UM)]$ suit une loi de chie-deux à m dégré de liberté
  -  Où RM et UM sont respectivement le modèle restreint et le modèle non restreint, m est le nombre de contraintes

## Test d'hypothèses

- Modèle non contraint

![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/unrestricted.png)

## Test d'hypothèses

- Modèle contraint

![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/restricted.png)


## Test d'hypothèses

- Test de nombreuses hypothèses
- Exemple: mortalité infantile

- Modèle sans restriction: probit mort jumelle femelle age15_19 age35_49 parity1 parity6 bambara primaire secondaire
  - L(UM) = -2315,8856
- Modèle restreint: Modèle restreint: jumeau mort probit femelle age15_19 age35_49 parity1 parity6 bambara
  - L(RM) = -2319,2534
- m = 2

- LR = -2*(-2319,2534 +2315,8856) = 6,74
- Chi-deux lu = 5,99 pour un niveau de signification de 5%
- Conclusion: Nous rejetons l'hypothèse nulle. L'éducation a un effet significatif sur la mortalité infantile.

## Extension

- La variable dépendante comprend plus de deux catégories:
  - Probit / logit ordonné
  - Il existe un classement clair entre les modalités
  - Ex. Quintile de richesse
- Logit / probit multinomial
  - Pas d'ordre, mais groupe distinct
  - Ex. Statut de travail (non, formel, informel)